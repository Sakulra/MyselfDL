@1.conda的操作
  1. conda --version #查看conda版本，验证是否安装
  2. conda update conda #更新至最新版本，也会更新其它相关包
  3. conda update --all #更新所有包
  4. conda update package_name #更新指定的包
  5. conda create -n env_name package_name #创建名为env_name的新环境，并在该环境下安装名为package_name 的包，
   可以指定新环境的版本号。例如：conda create -n python2 python=python2.7 numpy pandas，
   创建了python2环境，python版本为2.7，同时还安装了numpy pandas包
   也可conda create --name d2l python=3.9 -y 就单纯创建一个名叫d2l的环境，并装有3.9的python
  6. conda activate env_name #切换至env_name环境
  7. conda deactivate #退出环境
  8. conda info -e #显示所有已经创建的环境
  9. conda create --name new_env_name --clone old_env_name #复制old_env_name为new_env_name
  10. conda remove --name env_name –all #删除环境
  11. conda list #查看所
  有已经安装的包
  12. conda install package_name #在当前环境中安装包
  13. conda install --name env_name package_name #在指定环境中安装包
  14. conda remove -- name env_name package #删除指定环境中的包
  15. conda remove package #删除当前环境中的包
  16. conda env remove -n env_name #采用第10条的方法删除环境失败时，可采用这种方法
@2.将远程端口映射到本地，这样本地就可以打开远端的地址了
   ssh -L8888:localhost:8888 ubuntu@用户名
@3.norm默认求L2范数
@4.神经网络层数不计入输入层。每个输入都与每个输出相连，我们将这种变换称为全连接层（fully-connected layer）
   或称为稠密层（dense layer）
@5.要看某个向量或者矩阵a的形状时，a.shape,没有括号，加括号的时候是shape([1]),括号里面是要得到形状的对象，错好几次了。
@6.如果a是3x2的矩阵，len(a)返回的是矩阵的行数。
@7.batch_size其实越小对损失函数的收敛越有益，因为对于深度神经网络是很复杂的，当batch_size比较小的时候，
  在同一批次中噪音比例就会变大，而噪音对神经网络是有好处的，它可以增加算法鲁棒性
@8.全连接层在Linear类中定义；计算均方误差使用的是MSELoss类；SGD(小批量随机梯度下降算法)在optim模块中实现；
2.矩阵a的截取：a[起始行：结束行，起始列：结束列]，如果只截取某2列为a[,1]，因为第二列的索引是1，
   a[2]只截取第三行
   还有a[起始行：结束行：间隔行，起始列：结束列：间隔列],如a[::3,::2]，从第零行开始，每三行一跳，每两列一跳


3.矩阵切片loc和iloc。loc函数：通过行索引 "Index" 中的具体值来取行数据（如取"Index"为"A"的行）
   iloc函数：通过行号来取行数据（如取第二行的数据）。
#创建一个Dataframe
data=pd.DataFrame(np.arange(9).reshape(3,3),index=list('abc'),columns=list('ABC'))
Out: 
    A   B   C
a   0   1   2
b   4   5   6
c   8   9  10
#取索引为'a'的行
In: data.loc['a']
Out:
A    0
B    1
C    2
#取第一行数据，索引为'a'的行就是第一行，所以结果相同
In: data.iloc[0]
Out:
A    0
B    1
C    2
In:data.loc[['a','b'],['A','B']] #提取index为'a','b',列名为'A','B'中的数据
Out: 
   A  B
a  0  1
b  4  5
In:data.iloc[[0,1],[0,1]] #提取第0、1行，第0、1列中的数据
Out: 
   A  B
a  0  1
b  4  5

4.item()的作用是取出单元素张量的元素值并返回该值，保持该元素类型不变。
  a=torch.tensor([3.5])
  a.item()
  就可以得到数值3.5而不是向量[3.5]

5.os.path.join(path, *paths)
  path：代表文件系统路径的path-like对象。
  *paths：代表文件系统路径的path-like对象。它表示要连接的路径组件。
  从后往前看，会从第一个以'/'开头的参数开始拼接，之前的参数全部丢弃；以上一种情况为先。在上一种情况确保情况下，
  若出现'./'开头的参数，会从'./'开头的参数的前面参数全部保留。如果最后一个组件为空，则生成的路径以一个’/’分隔符结尾。

   print("1:", os.path.join('aaaa', '/bbbb', 'ccccc.txt'))
   print("2:", os.path.join('/aaaa', '/bbbb', '/ccccc.txt'))
   print("3:", os.path.join('aaaa', 'bbbb', './cccc', 'ccccc.txt'))
   print("4:", os.path.join('aaaa', 'bbbb', './cccc', '/dddd', 'ccccc.txt'))
   print("5:", os.path.join('aaaa', 'bbbb', 'cccc', 'dddd', 'ccccc.txt'))
   print("6:", os.path.join('aaaa', 'bbbb', 'cccc', 'dddd', ''))
   输出结果为：
   1: /bbbb/ccccc.txt
   2: /ccccc.txt
   3: aaaa/bbbb/./cccc/ccccc.txt
   4: /dddd/ccccc.txt
   5: aaaa/bbbb/cccc/dddd/ccccc.txt
   6: aaaa/bbbb/cccc/dddd/
6.os.mkdir(path,mode),mode就是文件权限4，2，1，代表读，写，执行。
  mkdir只能创建一个目录，如果路径中存在两个要创建的目录会报错
  os.makedirs(path,mode，exist_ok=False)递归创建目录，exist_ok：如果已经存在怎么处理，默认是 False ，
  即：已经存在程序报错。当为 True 时，创建目录的时候如果已经存在就不报错。
7.pytorch创建的张量默认是行向量，0轴--行，1轴--列，2轴--深度
8.torch.noraml(means, std, out=None)返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数。 
  均值means是一个张量，包含每个输出元素相关的正态分布的均值。 std是一个张量，
  包含每个输出元素相关的正态分布的标准差。 均值和标准差的形状不须匹配，但每个张量的元素个数须相同。
9.torch.matmul(input, other) → Tensor
  计算两个张量input和other的矩阵乘积
 【注意】：matmul函数没有强制规定维度和大小，可以用利用广播机制进行不同维度的相乘操作。
10.如果将Matplotlib绘图和我们平常画画相类比，可以把Figure想象成一张纸（一般被称之为画布），Axes代表的则是纸中的一片区域，axis是轴。
  plt绘图：第一种方式的代码来看，先生成了一个Figure画布，然后在这个画布上隐式生成一个画图区域进行画图。
  plt.figure()
  plt.plot([1,2,3],[4,5,6])
  plt.show()
  ax绘图：第二种方式同时生成了Figure和axes两个对象，然后用ax对象在其区域内进行绘图
  fig,ax = plt.subplots()
  ax.plot([1,2,3],[4,5,6])
  plt.show()
  也就是说fig, ax = plt.subplots()是fig = plt.figure()
                                  ax = fig.add_subplot(111)
   的简写

  plot()：用于绘制线图和散点图
  scatter()：用于绘制散点图
  bar()：用于绘制垂直条形图和水平条形图
  hist()：用于绘制直方图
  pie()：用于绘制饼图
  imshow()：用于绘制图像
  subplot():用于创建子图
  subplots()：用于创建子图

  plt.scatter(x, y, s=None, c=None, marker=None,alpha=None, linewidths=None,edgecolors=None)
  x, y → 散点的坐标
  s → 散点的面积,也就是画的每个点的面积
  c → 散点的颜色（默认值为蓝色，'b'，其余颜色同plt.plot( )）
  marker → 散点样式（默认值为实心圆，'o'，其余样式同plt.plot( )）
  alpha → 散点透明度（[0, 1]之间的数，0表示完全透明，1则表示完全不透明）
  linewidths →散点的边缘线宽
  edgecolors → 散点的边缘颜色
  详情：https://zhuanlan.zhihu.com/p/111331057

  ax = plt.gca() #获取当前坐标轴
  ax.set_title('第一张图')
  ax.set_ylabel('T(C)')
  ax.set_xlabel('Time')
  ax.legend()
  ax.axes.get_xaxis().set_visible(False)隐藏x坐标轴
  ax.axes.get_yaxis().set_visible(False)隐藏y坐标轴

  ax.flatten()是对多维数据的降维函数。
  ax.flatten(),默认缺省参数为0，也就是说flatten()和flatte(0)效果一样。python里的flatten(dim)表示，从第dim个维度开始展开，
  将后面的维度转化为一维.也就是说，只保留dim之前的维度，其他维度的数据全都挤在dim这一维。
11.
  画布参数plt.figure：
  fig=plt.figure(figsize=(4,3),facecolor='blue')  # 4*3 英寸，蓝色背景

  图例注释plt.legend()让标签显示出来：
  plt.legend(loc=3) #写3和'lower left'都可以

  创建子图subplot或subplots：
  区别：subplots() 既创建了一个包含子图区域的画布，又创建了一个 figure 图形对象，而 subplot() 只是创建一个包含子图区域的画布。
  subplot(nrows, ncols, index)
  subplots(nrows, ncols,sharex=False, sharey=False，figsize)
  示例：
  plt.subplot(221) #两行两列的第1个子图
  plt.plot(x,y)对第一个子图进行绘画
  plt.subplot(222) #两行两列的第2个子图
  plt.plot(x,y)对第二个子图进行绘画

  fig, axs = plt.subplots(2, 2)
  axs[0][0].plot(x, y)对第一个子图进行绘画
  axs[1][1].scatter(x, y)对第四个子图进行绘画
  不想用axs[,]可以
  f, ([ax1, ax2],[ax3,ax4]) = plt.subplots(2, 2, sharex=True)
  直接指定每一块区域的名字，然后直接用ax1，ax2，ax3，ax4来指定操作区域即可
12.向tensor矩阵传递向量索引，可以得到向量索引中的每个元素对应矩阵的行，A=tensor([[1,2,3],[4,5,6],[7,8,9]]),
   b=tensor([0,1]),c=tensor([1,2])，d=tensor([0,2])
   A[b]:tensor([[1,2,3],[4,5,6]])，得到A矩阵的第零行和第一行
   A[c]:tensor([[4,5,6],[7,8,9]])
   A[d]:tensor([[1,2,3],[7,8,9]])，得到矩阵A的第零行和第二行
13.with torch.no_grad():  表示所有计算得出的tensor的requires_grad都自动设置为False，反向传播时就不会自动求导了，
   从而节省计算开支。
14.data.TensorDataset() 以用来对tensor进行打包，就好像python中的zip功能。该类通过每一个tensor的
   第一个维度进行索引。因此，该类中的 tensor 第一维度必须相等. 另外：TensorDataset 中的参数必须是 tensor.
15.zip()函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。
   如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表
   a = [1,2,3]
   b = [4,5,6,7,8]
   zipped = zip(a,b)     # 打包为元组的列表
输出为：[(1, 4), (2, 5), (3, 6)]
   zip(*zipped)    # 与 zip 相反，*zipped 可理解为解压，返回二维矩阵式
输出为：[(1, 2, 3), (4, 5, 6)]
16.data.DataLoader()就是用来包装所使用的数据，把训练数据分成多个小组，每次抛出一批数据.
   a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9]])
   b = torch.tensor([44, 55, 66, 44, 55, 66, 44, 55, 66])
   # TensorDataset对tensor进行打包
   train_ids = TensorDataset(a, b) 
   for x_train, y_label in train_ids:
      print(x_train, y_label)
输出结果为：
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)

# dataloader进行数据封装,就是拿出批次大小的数据里面的内容就是dataset()后的形式一般是[(数据1，标签1)，(数据2，标签2),]
train_loader = DataLoader(dataset=train_ids, batch_size=3, shuffle=True)
for i, data in enumerate(train_loader, 1):  
# 注意enumerate返回值有两个,一个是序号，一个是数据（包含训练数据和标签）
    x_data, label = data
    print(' batch:{0} x_data:{1}  label: {2}'.format(i, x_data, label))
输出结果为：
 batch:1 x_data:tensor([[7, 8, 9],
        [1, 2, 3],
        [4, 5, 6]])  label: tensor([66, 44, 55])
 batch:2 x_data:tensor([[4, 5, 6],
        [4, 5, 6],
        [1, 2, 3]])  label: tensor([55, 55, 44])
 batch:3 x_data:tensor([[1, 2, 3],
        [7, 8, 9],
        [7, 8, 9]])  label: tensor([44, 66, 66])
17.独热编码（one-hot encoding）,独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。
18.交叉熵损失（cross-entropy loss），它是分类问题最常用的损失之一。交叉熵从P到Q，记为H(P,Q)。可以把交叉熵想象为“主观概率
   为Q的观察者在看到根据概率P生成的数据时的预期惊异”。当P=Q时，交叉熵达到最低。交叉熵是一个衡量两个概率分布之间差异的很好的度量，
   它测量给定模型编码数据所需的比特数。
19.iter(object)用来生成迭代器，迭代器是一个可以记住遍历的位置的对象。object -- 支持迭代的集合对象。
   一类是：list、tuple、dict、set、str。二类是：generator（都是Iterator对象），包含生成器和带yield的generator function
   生成器不但可以作用于for，还可以被next函数不断调用并且返回下一个值，可以被next函数不断调用返回下一个值的对象称为迭代器（Iterator）。
20.next(iterator),iterator--可迭代的对象,iter()把目标转为可迭代的对象，然后通过next(iterator)访问每一个可迭代的对象。
