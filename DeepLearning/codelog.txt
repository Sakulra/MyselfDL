@1.conda的操作
  1. conda --version #查看conda版本，验证是否安装
  2. conda update conda #更新至最新版本，也会更新其它相关包
  3. conda update --all #更新所有包
  4. conda update package_name #更新指定的包
  5. conda create -n env_name package_name #创建名为env_name的新环境，并在该环境下安装名为package_name 的包，
   可以指定新环境的版本号。例如：conda create -n python2 python=python2.7 numpy pandas，
   创建了python2环境，python版本为2.7，同时还安装了numpy pandas包
   也可conda create --name d2l python=3.9 -y 就单纯创建一个名叫d2l的环境，并装有3.9的python
  6. conda activate env_name #切换至env_name环境
  7. conda deactivate #退出环境
  8. conda info -e #显示所有已经创建的环境
  9. conda create --name new_env_name --clone old_env_name #复制old_env_name为new_env_name
  10. conda remove --name env_name –all #删除环境
  11. conda list #查看所有已经安装的包
  12. conda install package_name #在当前环境中安装包
  13. conda install --name env_name package_name #在指定环境中安装包
  14. conda remove -- name env_name package #删除指定环境中的包
  15. conda remove package #删除当前环境中的包
  16. conda env remove -n env_name #采用第10条的方法删除环境失败时，可采用这种方法
@2.将远程端口映射到本地，这样本地就可以打开远端的地址了
   ssh -L8888:localhost:8888 ubuntu@用户名
@3.norm默认求L2范数
@4.神经网络层数不计入输入层。每个输入都与每个输出相连，我们将这种变换称为全连接层（fully-connected layer）
   或称为稠密层（dense layer）
@5.要看某个向量或者矩阵a的形状时，a.shape,没有括号，因为shape是属性，可用中括号读取指定维度的大小,shape[1]就是读取轴1的形状。
   比如二维矩阵shape[0]是行数，shape[1]是列数，shape[-1]是列数，shape[-2]是行数
   也可项shape传递具体矩阵这时候要加括号shape([1,2]).
   a.size()也可查看形状，size是函数所以要加括号。

   维度(dim)和轴(axis)，只有指定的维度是可变的，其他都是固定的，numpy用axis，pytorch用dim，二者其实是一样的
     axis：0轴从第一层向最深层方向
         1轴沿着0行到1行的方向垂直向下(纵向),按行计算(列不变)，得到列的性质；
         2轴沿着0列到1列的方向水平延伸(横向),按列计算，得到行的性质
     dim：正数就从外面往里数中括号，第一层是dim=0，负数就从里往外数。
@6.如果a是3x2的矩阵，len(a)返回的是矩阵的行数。矩阵[p,m,n]其中p代表p个切片，m代表m行，n代表n列。
@7.batch_size其实越小对损失函数的收敛越有益，因为对于深度神经网络是很复杂的，当batch_size比较小的时候，
  在同一批次中噪音比例就会变大，而噪音对神经网络是有好处的，它可以增加算法鲁棒性
@8.全连接层在Linear类中定义；计算均方误差使用的是MSELoss类；SGD(小批量随机梯度下降算法)在optim模块中实现；
@9.损失函数和优化函数的联合过程：损失函数仅仅用来计算预测与真实之间的差距大小，而优化函数可以调整拟合函数从而使损失函数的值变小。
   对于一个优化函数，要优化的是拟合函数中的某些变量，比如截距b，我们可以先初始化b为0，然后计算得到b=0时损失函数的具体值，
   (横坐标的b，纵坐标是损失函数总值)通过对损失函数进行求导得到b=0时的斜率，然后对斜率乘以一个学习率，用b-lr*斜率，就可以得到一个新的b，
   然后使用新的b，又可以计算得到一个具体的损失函数值，然后...   讲解https://www.bilibili.com/video/BV1XW421F7M4
@10.当我们在一个对象上调用内置的len()函数时，实际上是在调用该对象的__len__()方法。这个方法的主要作用是返回一个对象的长度。
@11.model.eval()将模型设为评估模式。在评估模式下，模型的所有层都将正常运行，但不会进行反向传播（backpropagation）和参数更新。
   此外，某些层的行为也会发生改变，如Dropout层将停止dropout，BatchNorm层将使用训练时得到的全局统计数据而不是评估数据集中的批统计数据
   model.zero_grad()的作用是将所有模型参数的梯度置为0。
   optimizer.zero_grad()的作用是清除所有可训练的torch.Tensor的梯度。
   torch.no_grad()是PyTorch的一个上下文管理器,用于在不需要计算梯度的场景下禁用梯度计算.

   函数后面加下划线'_',意思是对tensor进行in-place类型操作，也就是函数会直接修改输入的Tensor，而不创建新的Tensor对象。
@12.为对抗过拟合的技术称为正则化（regularization）
   正则化的作用：防止过拟合、提高模型的泛化能力、减小模型复杂度、控制模型的学习速率、提高模型的鲁棒性(对输入微小变化不敏感)。
   常见正则化：L1正则化、L2正则化、Elastic Net正则化、Dropout、早停、数据增强、批量归一化、权重衰减。
   L1：在模型的损失函数中增加权重的 L1 范数来实现正则化。L1 正则化倾向于产生稀疏权重矩阵，即将一些权重推向零，从而实现特征选择的效果。
   L2：使用L2范数正则化的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。
       在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，L1惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零,这称为特征选择。
   Elastic Net:是 L1 和 L2 正则化的组合,它在损失函数中同时使用 L1 和 L2 范数，可以综合两者的优点.
   Dropout：随机丢弃隐藏中的节点
   早停：它在训练过程中监视模型在验证集上的性能，一旦验证集上的性能开始下降，就停止训练。
   数据增强：通过对训练数据进行变换来增加数据的多样性，从而减少过拟合的风险。例如，在图像分类任务中可以进行随机裁剪、旋转、翻转等操作来增加训练数据的数量。
   批量归一化：对每个批次的输入进行归一化来加速训练并减少过拟合的技术。
   权重衰减：在损失函数中增加惩罚项，加L1就是L1正则化，加L2就是L2正则化。
@13.暂退法(dropout)(应用于全连接层)：在计算后续层之前向网络的每一层注入噪声。因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。
   dropout只在训练集上应用而不应用于测试集，dropout是为了防止过拟合，而在测试集中应该用训练好的整个网络。
   BN(应用于卷积层)
@14.影响模型泛化的因素:
   1. 可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合
   2. 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
   3. 训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。
   为什么限制参数的取值范围模型复杂度就低呢？
   因为如果参数可以取很大很小，那么函数就可以尽可能地靠近每一个训练数据，那么学习出的函数就会不怎么平滑，同时此函数必然会更复杂。
@15.自定义网络时class类里面的forward函数有什么用？
    它是自定义网络的核心之一，自定义时在__init__函数中初始化各种网络参数，然后在forward函数中编写各网络的关系，即实现各个层之间的连接关系。
@16.正向传播：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。
    反向传播：是计算神经网络参数梯度的方法。根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。
@17.object类型是其他类诸如int64，float64，str类的基类，其他类都继承object类
@18.在编写函数时如果要对某些参数设置默认值，那么这个参数之后的参数也必须设置默认值，否则报错。或者最后写这个需要默认值的参数。
    有些python版本会把NA和None都视为缺失值，这导致None类型被错误理解为缺失值。
@19.查看显卡信息：nvidia-smi
    使用cpu计算：torch.device('cpu')
    使用第0个gpu计算：torch.cuda.device('cuda')
    使用第1个gou计算：torch.cuda.device('cuda:1')
    查询gpu的数量：torch.cuda.device_count()
    将net网络移到gpu上：net.to(device = 'cuda'),即将输入和权重都copy到gpu上。
    都是复制tensor.cuda()与net.to(device)的区别：to(device)时Module的东西，Module只能用to(device)
@20.一般在使用gpu训练时，一般在net前把data移到gpu上
@21.填充一般是奇数，步幅一般是偶数，核得大小一般是奇数
    核大小，填充步幅对网络的影响：填充一般是核大小-1，这样输出的形状大小就会和输入大小一样，便于记忆，也不用考虑输出形状的变化了。
    步幅1是最好的，当觉得计算量太大了，就会设步幅不为1.
    卷积神经网络不能每次都让输入维度减半，这样的话就没办法做深的神经网络了，所以有使输出和输入的维度不变的处理。
    网络越深，padding 0越多，实际上对网络的精度没有影响。
@22.现在已经有很多经典的网络结构，我们平常使用的时候无需自己设计卷积核，直接套用经典的结构就可。其实网络结构没有想象的那么关键
    一般的kernel选择3*3，它的视野是很小，但随着网络层数的增加最后的那个3*3里的数对应到第一层的区域就会很大了。
    也就是说一般小核配合深得神经网络。
    多层的3*3的卷积，在效果上是可以用更少层的5*5卷积代替，但是3*3的卷积会更快.
    当输出的高宽与输入的高宽一样的话，一般不改变输出通道数；当输出高宽减半时，通道数变为2倍，直观理解是当空间信息压缩了，
    就需要更多的通道来存储信息。
@23.机器学习实际上就是一个压缩算法，将输入压缩成最后的几个输出的值，那么它必定会丢失信息。
@24.Tensor存储机制：tensor在电脑的储存，分为两个部分(也就是说一个tensor占用了两个内存位置)，
    一个内存储存了这个tensor的形状size、步长stride、数据的索引等信息，我们把这一部分称之为头信息区（Tensor）；
    另一个内存储的就是真正的数据，我们称为存储区 （Storage）。
@25.在矩阵相乘且需要降维的时候，一般是通过W·X进行形状设计，而不是根据X·W进行形状设计。因为进行reshape维数的时候不利于X·W。
@26.不同得核对同意输入可以得到不同的效果，比如锐化、模糊、边缘检测，但是这些不同的核的参数是学习出来的，不是设计出来的。

2.矩阵a的截取：a[起始行：结束行，起始列：结束列]，如果只截取某2列为a[,1]，因为第二列的索引是1，
   a[2]只截取第三行
   还有a[起始行：结束行：间隔行，起始列：结束列：间隔列],如a[::3,::2]，从第零行开始，每三行一跳，每两列一跳.
   高级索引:在numpy或pytorch等框架中对张量的操作不只有类似列表的切片索引等操作，还提供了高级索引,整数数组索引、布尔索引及花式索引。
   整数数组索引：
   x=[[0,1,2]
      [3,4,5]
      [6,7,8]]
   1.取出其(0,0)，(1,1)和(2,0)位置处的元素,给出两个一维数组分别指定第0维和第1维,x[[0,1,2],[0,1,0]]
   2.传入的位置指定数组也可为多维，取出其[[(0,0),(0,2)]，[(3,0),(3,2)]]的元素
   rows = torch.tensor([[0,0],[3,3]]) 
   cols = torch.tensor([[0,2],[0,2]])
   y = x[rows,cols]

3.矩阵切片loc[]和iloc[]。loc函数：通过行索引 "Index" 中的具体值来取行数据（如取"Index"为"A"的行）
   iloc函数：通过行号来取行数据（如取第二行的数据）。
#创建一个Dataframe结构的数据
data=pd.DataFrame(np.arange(9).reshape(3,3),index=list('abc'),columns=list('ABC'))
Out: 
    A   B   C
a   0   1   2
b   3   4   5
c   6   7  8
#取索引为'a'的行
In: data.loc['a']
Out:
A    0
B    1
C    2
#取第一行数据，索引为'a'的行就是第一行，所以结果相同
In: data.iloc[0]
Out:
A    0
B    1
C    2
In:data.loc[['a','b'],['A','B']] #提取index为'a','b',列名为'A','B'中的数据
Out: 
   A  B
a  0  1
b  3  4
In:data.iloc[[0,1],[0,1]] #提取第0、1行，第0、1列中的数据
或者data.iloc[0:1,0:1]
Out: 
   A  B
a  0  1
b  3  4
对于特别大的矩阵也有取前四行的正数第1，2，3，4列和倒数1，2，3，4列，data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]

4.item()的作用是取出单元素张量的元素数值并返回该数值，保持原元素类型不变。
  a=torch.tensor([3.5])
  a.item()
  就可以得到数值3.5而不是向量[3.5]

5.os.path.join(path, *paths)
  会自动在要连接的字符串之间添加'/'
  path：代表文件系统路径的path-like对象。
  *paths：代表文件系统路径的path-like对象。它表示要连接的路径组件。
  从后往前看，会从第一个以'/'开头的参数开始拼接，之前的参数全部丢弃；以上一种情况为先。在上一种情况确保情况下，
  若出现'./'开头的参数，会从'./'开头的参数的前面参数全部保留。如果最后一个组件为空，则生成的路径以一个’/’分隔符结尾。

  print("1:", os.path.join('aaaa', '/bbbb', 'ccccc.txt'))
  print("2:", os.path.join('/aaaa', '/bbbb', '/ccccc.txt'))
  print("3:", os.path.join('aaaa', 'bbbb', './cccc', 'ccccc.txt'))
  print("4:", os.path.join('aaaa', 'bbbb', './cccc', '/dddd', 'ccccc.txt'))
  print("5:", os.path.join('aaaa', 'bbbb', 'cccc', 'dddd', 'ccccc.txt'))
  print("6:", os.path.join('aaaa', 'bbbb', 'cccc', 'dddd', ''))
  输出结果为：
   1: /bbbb/ccccc.txt
   2: /ccccc.txt
   3: aaaa/bbbb/./cccc/ccccc.txt
   4: /dddd/ccccc.txt
   5: aaaa/bbbb/cccc/dddd/ccccc.txt
   6: aaaa/bbbb/cccc/dddd/
6.os.mkdir(path,mode),mode就是文件权限4，2，1，代表读，写，执行。
  mkdir只能创建一个目录，如果路径中存在两个要创建的目录会报错
  os.makedirs(path,mode，exist_ok=False)递归创建目录，exist_ok：如果已经存在怎么处理，默认是 False ，
  即：已经存在程序报错。当为 True 时，创建目录的时候如果已经存在就不报错。
7.pytorch创建的张量默认是行向量，0轴--行，1轴--列，2轴--深度
8.torch.noraml(means, std, out=None)返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数。 
  均值means是一个张量，包含每个输出元素相关的正态分布的均值。 std是一个张量，
  包含每个输出元素相关的正态分布的标准差。 均值和标准差的形状不须匹配，但每个张量的元素个数须相同。
9.矩阵运算@，*，tensor.dot()，torch.mul()，torch.mm()，torch.mv()，tensor.t()，torch.matmul()
  @表示常规的数学上定义的矩阵相乘
  *表示两个矩阵对应位置处的两个元素相乘
  torc.dot(x,y): x和y对应位置相乘最后相加,x，y均为一维向量，和np.dot()不同，不能是多维矩阵，
  torch.mul()等同*:表示相同shape矩阵点乘，即对应位置相乘，得到矩阵有相同的shape，可用广播机制。
  torch.mm(a, b)等同@：正常矩阵相乘，要求a的列数与b的行数相同。不会进行广播
  torch.mv(X, w0):矩阵和向量相乘.第一个参数是矩阵，第二个参数只能是一维向量,等价于X乘以w0的转置。
  Y.t():矩阵Y的转置。
  torch.matmul(input, other) → Tensor，可用于pytorch绝大多数乘法。计算两个张量input和other的矩阵乘积，可利用广播机制进行不同维度的相乘操作。
  torch.sum(input,dim)对向量的指定维度进行求和,得到的还是一个tensor
10.tensor.detach()返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,
   不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。
   将有梯度的tensor转化位numpy用tensor.detach().numpy()
   将没梯度的tensor转换为numpy直接用tensor.numpy()
   注意转换后的numpy与tensor指向同一个地址，改变一个另一个也会改变。
11.matplotlib允许我们将一个figure通过栅格系统划分成不同的格子，然后在格子中画图，这样就可以在一个figure中画多个图了。
  这里的每个格子有两个名称：Axes和subplot。subplot是从figure所有的格子来看的。因为figure要统一管理协调这些格子的位置、
  间隔等属性，管理协调的方法和属性设置就在subplots的层面进行。Axes是从作为画图者的我们的角度来定义的，我们要画的点、线等
  都在Axes这个层面来进行。画图用的坐标系统自然也是在Axes中来设置的。Axes数组是np.array类型。
  如果将Matplotlib绘图和我们平常画画相类比，可以把Figure想象成一张纸（一般被称之为画布），Axes代表的则是纸中的一片区域，axis是轴。
  plt绘图：第一种方式的代码来看，先生成了一个Figure画布，然后在这个画布上隐式生成一个画图区域进行画图。
  plt.figure()
  plt.plot([1,2,3],[4,5,6])
  plt.show()
  ax绘图：第二种方式同时生成了Figure和axes两个对象，然后用ax对象在其区域内进行绘图
  fig,ax = plt.subplots()
  ax.plot([1,2,3],[4,5,6])
  plt.show()
  也就是说fig, ax = plt.subplots()是fig = plt.figure()
                                  ax = fig.add_subplot(111)
   的简写

  plot()：用于绘制线图和散点图
  scatter()：用于绘制散点图
  bar()：用于绘制垂直条形图和水平条形图
  hist()：用于绘制直方图
  pie()：用于绘制饼图
  imshow()：用于绘制图像
  subplot():用于创建子图
  subplots()：用于创建子图

  plt.scatter(x, y, s=None, c=None, marker=None,alpha=None, linewidths=None,edgecolors=None)
  x, y → 散点的坐标
  s → 散点的面积,也就是画的每个点的面积
  c → 散点的颜色（默认值为蓝色，'b'，其余颜色同plt.plot( )）
  marker → 散点样式（默认值为实心圆，'o'，其余样式同plt.plot( )）
  alpha → 散点透明度（[0, 1]之间的数，0表示完全透明，1则表示完全不透明）
  linewidths →散点的边缘线宽
  edgecolors → 散点的边缘颜色
  xlim,ylim → 设置x轴和y轴的极限
  fmt = '[marker][line][color]' 
      https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html
       线型  '-'：实线  '--'：虚线  '-.'：点画线  ':'：点线
       点型  
       颜色
  详情：https://zhuanlan.zhihu.com/p/111331057

  ax = plt.gca() #获取当前坐标轴
  ax.set_title('第一张图')
  ax.set_ylabel('T(C)')
  ax.set_xlabel('Time')
  ax.legend()
  ax.axes.get_xaxis().set_visible(False)隐藏x坐标轴
  ax.axes.get_yaxis().set_visible(False)隐藏y坐标轴

  ax.flatten()是对多维数据的降维函数。
  ax.flatten(),默认缺省参数为0，也就是说flatten()和flatten(0)效果一样。python里的flatten(dim)表示，从第dim个维度开始展开，
  将后面的维度转化为一维.也就是说，只保留dim之前的维度，其他维度的数据全都挤在dim这一维。
12.
  画布参数plt.figure：
  fig=plt.figure(figsize=(4,3),facecolor='blue')  # 4*3 英寸，蓝色背景

  plt.ioff()  # 关闭画图的窗口
  plt.cla  #清除上一幅图像

  图例注释plt.legend()让标签显示出来：
  plt.legend(loc=3) #写3和'lower left'都可以

  创建子图subplot或subplots：
  区别：subplots() 既创建了一个包含子图区域的画布，又创建了一个 figure 图形对象，而 subplot() 只是创建一个包含子图区域的画布。
  subplot(nrows, ncols, index)index表示在第index个区域进行操作，不管其他区域，索引按照从左到右从上到下编号。。
  subplots(nrows, ncols,sharex=False, sharey=False，figsize)
  示例：
  plt.subplot(221) #两行两列的第1个子图
  plt.plot(x,y)对第一个子图进行绘画
  plt.subplot(222) #两行两列的第2个子图
  plt.plot(x,y)对第二个子图进行绘画

  fig, axs = plt.subplots(2, 2)
  axs[0]代表的就是第一行两个子区域
  axs[0][0].plot(x, y) 或 axs[0,0]  对第一个子图进行绘画
  axs[0,1].plot(x,y) 对第二个子图进行绘画
  axs[1][1].scatter(x, y) 或 axs[1,1].scatter(x,y) 对第四个子图进行绘画
  不想用axs[,]可以
  f, ([ax1, ax2],[ax3,ax4]) = plt.subplots(2, 2, sharex=True)
  直接指定每一块区域的名字，然后直接用ax1，ax2，ax3，ax4来指定操作区域即可
13.向tensor矩阵传递向量索引，可以得到向量索引中的每个元素对应矩阵的行，A=tensor([[1,2,3],[4,5,6],[7,8,9]]),
   b=tensor([0,1]),c=tensor([1,2])，d=tensor([0,2])
   A[b]:tensor([[1,2,3],[4,5,6]])，得到A矩阵的第零行和第一行
   A[c]:tensor([[4,5,6],[7,8,9]])
   A[d]:tensor([[1,2,3],[7,8,9]])，得到矩阵A的第零行和第二行
14.with torch.no_grad():  表示所有计算得出的tensor的requires_grad都自动设置为False，反向传播时就不会自动求导了，从而节省计算开支。
15.zip()函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。
   如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表
   a = [1,2,3]
   b = [4,5,6,7,8]
   zipped = zip(a,b)     # 打包为元组的列表
输出为：[(1, 4), (2, 5), (3, 6)]
   zip(*zipped)    # 与 zip 相反，*zipped 可理解为解压，返回二维矩阵式
输出为：[(1, 2, 3), (4, 5, 6)]
16.data.TensorDataset() 以用来对tensor进行打包，就好像python中的zip功能。该类通过每一个tensor的第一个维度进行索引。
   因此，该类中的 tensor 第一维度必须相等. 另外：TensorDataset 中的参数必须是 tensor.
17.TensorDataset(tensor1,tensor2)-->[[tensor1，label1]，[tensor2,label2],...]
   from torch.utils import data
   a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9]])
   b = torch.tensor([44, 55, 66, 44, 55, 66, 44, 55, 66])
   # TensorDataset对tensor进行打包
   train_ids = data.TensorDataset(a, b)
   #打包后的列表里的每一个对象的形式为(tensor([4, 5, 6]), tensor(55))
   for x_train, y_label in train_ids:
      print(x_train, y_label)
输出结果为：
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)

18.data.DataLoader()就是用来包装所使用的数据，把训练数据分成多个小组，每次抛出一批数据.
dataloader进行数据封装,就是拿出批次大小的数据，里面的内容类似于dataset()后的形式，batch_size=1时是[(数据1，标签1)，(数据2，标签2),]，
batch_size=2时是[[tensor[数据1，数据2]，tensor[标签1，标签2]]，tensor[[数据3，数据4]，tensor[标签3，标签4]]]
即一个对象(是个列表)里面包含batch_size个样本，数据又分为数据向量矩阵和标签向量矩阵
[tensor([[4, 5, 6],
        [4, 5, 6],
        [1, 2, 3]]), tensor([55, 55, 44])]
train_loader = DataLoader(dataset=train_ids, batch_size=3, shuffle=True)
for i, data in enumerate(train_loader, 1):  
#enumerate(sequence,[start=0])，默认序号从0开始数，就比如这个指定了1，就从batch：1即从序号1开始数了，实际并不影响数据，仅仅只是计数而已。
# 注意enumerate返回值有两个,一个是序号，一个是数据（包含训练数据和标签）。
    x_data, label = data
    print(' batch:{0} x_data:{1}  label: {2}'.format(i, x_data, label))
输出结果为：
 batch:1 x_data:tensor([[7, 8, 9],
        [1, 2, 3],
        [4, 5, 6]])  label: tensor([66, 44, 55])
 batch:2 x_data:tensor([[4, 5, 6],
        [4, 5, 6],
        [1, 2, 3]])  label: tensor([55, 55, 44])
 batch:3 x_data:tensor([[1, 2, 3],
        [7, 8, 9],
        [7, 8, 9]])  label: tensor([44, 66, 66])
19.独热编码（one-hot encoding）,独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。
20.交叉熵损失（cross-entropy loss），它是分类问题最常用的损失之一。交叉熵从P到Q，记为H(P,Q)。可以把交叉熵想象为“主观概率
   为Q的观察者在看到根据概率P生成的数据时的预期惊异”。当P=Q时，交叉熵达到最低。交叉熵是一个衡量两个概率分布之间差异的很好的度量，
   它测量给定模型编码数据所需的比特数。

   公式：
   先对每一个样本数据中的每一类别求 -(1类真实概率*log(1类预测概率)+2类真实概率*log(2类预测概率)+...)
   然后对每一个样本计算出的结果求和再平均

   sigmoid(softmax)+cross-entropy loss 擅长于学习类间的信息，因为它采用了类间竞争机制，它只关心对于正确标签预测概率的准确性，
   忽略了其他非正确标签的差异，导致学习到的特征比较散。基于这个问题的优化有很多，比如对softmax进行改进，
   如L-Softmax、SM-Softmax、AM-Softmax等
   详情：https://www.zhihu.com/tardis/zm/art/35709485?source_id=1005
21.iter(object)用来生成迭代器，迭代器是一个可以记住遍历的位置的对象。object -- 支持迭代的集合对象。
   一类是：list、tuple、dict、set、str。二类是：generator（都是Iterator对象），包含生成器和带yield的generator function
   生成器不但可以作用于for，还可以被next函数不断调用并且返回下一个值，可以被next函数不断调用返回下一个值的对象称为迭代器（Iterator）。
22.next(iterator),iterator--可迭代的对象,iter()把目标转为可迭代的对象，然后通过next(iterator)访问每一个可迭代的对象。
23.torch.view()相当于reshape、resize，重新调整Tensor的形状，只能对张量进行操作，不能对numpy操作
   v1 = torch.range(1, 16) 
   v2 = v1.view(4, 4)

   torch.reshape()、numpy.reshape()，reshape不仅能对张量操作，也能对numpy进行操作。
   reshape把高维 → 低维就是把dim=3的数据直接接到dim=2的下面
   reshape低维 → 高维就是把dim=2后面的数据截断，放到dim=3的地方。
   torch.resize()不推荐使用，使用不当它会改变数据


24.transforms.ToTensor()这是一个图像转换的操作，将图像从PIL(Python Imaging Library)格式转换为PyTorch的Tensor类型。
   它还会将像素值的范围从[0, 255]缩放到[0, 1]。
25.transforms.Compose()就是一个把几个对图像的操作连接成一个操作的函数。具体是对图像进行各种转换操作，并用函数compose将这些转换操作组合起来。
   它其实是一个列表，按顺序记录着各种转换操作，也称为转换列表。
26.torch.argmax(input，[dim]) → LongTensor。返回输入张量中指定维度所有元素的最大值的索引，并组成一维向量tensor([1,2,0,1,])，可以看成行向量。
   不指定维度就是把矩阵展成一维向量，再返回索引。
   实例https://zhuanlan.zhihu.com/p/409912530
27.numel()函数：返回数组中元素的个数。y.numel()
28.plt.draw()函数的作用是重新绘制当前图形。如果需要清除现有图形可以使用 matplotlib.pyplot.clf() 和 matplotlib.axes.Axes.clear()。
   如果在修改图形的过程中使用了plt.show()函数，则plt.draw()函数将失效。因此，一般情况下应该先调用plt.draw()函数，然后再使用plt.show()函数。
   plt.show()将显示您正在处理的当前图形。
29.ipython中display模块中的display函数它可以显示文本、图像、音频和视频等。
   from IPython.display import display,Image，Audio, Video
   显示图像：display(Image(filename='image.png'))
   显示视频：display(Video(filename='video.mp4'))
   显示音频：display(Audio(filename='audio.mp3'))
30.np.linspace(start,stop,num,[endpoint],[dtype]),起始值，终值，生成个数。生成结果第一个就是起始值，最后一个就是终值，其余数在它们之间等间隔取。
   可选参数endpoint决定输出结果包不包含终值，dtype决定输出数的类型。
   从0 到 100，间隔为10的数值序列：np.linspace(start = 0, stop = 100, num = 11)
   输出array([  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.])
31.lambda函数的语法只包含一个语句lambda[arg1 [,arg2,.....argn]]:expression。[arg…] 和 expression 由用户自定义。
   expression是一个参数表达式，表达式中出现的参数需要在[arg......]中有定义，并且表达式只能是单行的，只能有一个表达式。
   特性：
   lambda 函数是匿名的，通俗地说就是没有名字的函数。lambda函数没有名字。
   lambda 函数有输入和输出：输入是传入到参数列表argument_list的值，输出是根据表达式expression计算得到的值。
   lambda 函数拥有自己的命名空间：不能访问自己参数列表之外或全局命名空间里的参数，只能完成非常简单的功能。
   用法1：将lambda函数赋值给一个变量，通过这个变量间接调用该lambda函数。
   add = lambda x, y: x+y  相当于定义了加法函数lambda x, y: x+y，并将其赋值给变量add，这样变量add就指向了具有加法功能的函数。
   这时我们如果执行add(1, 2)，其输出结果就为 3。
   用法2：将lambda函数赋值给其他函数，从而将其他函数用该lambda函数替换。
   time.sleep=lambda x: None   #把标准库time中的函数sleep的功能屏蔽
   time.sleep(3)	# 程序不会休眠 3 秒钟，而是因为lambda输出为None，所以这里结果是什么都不做
32.map(function, iterable, ...)函数：将可迭代对象中的每一个对象调用function函数，然后返回所有function函数输出结果的迭代器(新列表)。
   def square(x):
	   return x ** 2
   map(square, [1,2,3,4,5])
   输出[1, 4, 9, 16, 25]
   也可改为lambda匿名函数写法：
   map(lambda x: x ** 2, [1, 2, 3, 4, 5])
   提供两个列表，将其相同索引位置的列表元素进行相加：map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])
33.reduce(function, iterable[, initializer])函数会对参数序列(可迭代对象)中的元素进行累积。
   函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）
   先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个元素用 function 函数运算，最后得到一个结果。
   from functools import reduce
   def add(x, y):            
	   return x + y
   reduce(add, [1, 3, 5, 7, 9])    # 计算列表元素和：1+3+5+7+9
   输出25
   也可改成lambda写法：
   reduce(lambda x, y: x + y, [1, 2, 3, 4, 5])
   当要把序列 [1, 3, 5, 7, 9] 变换成整数 13579，reduce就很好用：
   from functools import reduce
   def fn(x, y):
	   return x * 10 + y
   reduce(fn, [1, 3, 5, 7, 9])
   输出13579
34.sorted(iterable, key=None, reverse=False)
   sort和sorted的区别：sort是list的一个方法，而sorted可以对所有可迭代的对象进行排序操作。
   list 的 sort 方法返回的是对已经存在的列表进行操作，无返回值，而内建函数sorted方法返回的是一个新的list，而不是在原来的基础上进行的操作。
   key：用来进行比较的元素，只有一个参数，参数取自可迭代对象
   reverse：是排序规则，reverse = True 降序 ， reverse = False 升序（默认）
   L=[('b',2),('a',1),('c',3),('d',4)]
   sorted(L, key=lambda x:x[1])
   输出[('a', 1), ('b', 2), ('c', 3), ('d', 4)]
35.filter(function, iterable)用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。该接收两个参数，
   第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。
   newlist = filter(lambda x: x % 3 == 0, [1, 2, 3])
   print(list(newlist)) 输出[3]
36.hasattr(object, name)object → 对象；name → 字符串，属性名。
   用于判断对象是否包含对应的属性
37.assert expression,'自定义报错语句'，在表达式条件为false时触发异常。在条件不满足程序运行条件时直接返回错误，而不必等待程序运行后出现崩溃的情况.
   它主要是用来检测调试你的代码问题，当你使用 assert 来检测你的代码的时候，如果是 True ，它就会直接通过，
   当它是 False 的时候，就会抛出错误，然后你就可以根据错误进行定位，从而在具体的位置修改代码。
38.用于创建一个与已知 tensor 形状相同的 tensor:torch.*_like(),比如torch.zeros_like(input)和torch.ones_like(input)是生成一个和input同型的全0或全1矩阵.
   用于创建特殊形式的tensor：torch.*,比如torch.ones(),torch.zeros()
   用于创建一个与已知 tensor 类型相同的 tensor:torch.new_*
39.plt.rcParams[key]它是一个字典。使用rc配置文件可以自定义图形的各种默认属性，包括窗体大小、每英寸的点数、线条宽度、颜色、样式、坐标轴、
   坐标和网络属性、文本、字体等。rc参数存储在字典变量中，通过字典的方式进行访问。
40.x.grad.data.zero_(),optimizer.zero_grad()
41.optimizer.zero_grad和model.zero_grad的区别(model为自定义模型)：
   当仅有一个model，同时optimizer只包含这一个model的参数，那么model.zero_grad和optimizer.zero_grad没有区别，可以任意使用
   当有多个model，同时optimizer包含多个model的参数时，如果这多个model都需要训练，那么使用optimizer.zero_grad是比较好的方式，
   耗时和防止出错上比对每个model都进行zero_grad要更好。
   当有多个model，对于每个model或者部分model有对应的optimizer，同时还有一个total_optimizer包含多个model的参数时。
   如果是是只想训练某一个model或者一部分model，可以选择对需要训练的那个model进行model.zero_grad，然后使用他对应的optimizer进行优化。
   如果是想对所有的model进行训练，那么使用total_optimizer.zero_grad是更优的方式。
42.当向量没有进行过求导运算时进行梯度清零会报错AttributeError: 'NoneType' object has no attribute 'data'
43.nn.Sequential()一个有序的容器,可以看作列表。可通过net[idx]来访问里面的每一个模块。比如net[2].state_dict()、net[2].bias.data
   里面的神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。
   使用方法一：作为一个有顺序的容器
   model = nn.Sequential(
      nn.Conv2d(1,20,5),
      nn.ReLU(),
      nn.Conv2d(20,64,5),
      nn.ReLU()
      )
   net = nn.Sequential(
         nn.Linear(num_inputs, num_hidden)
    # 传入其他层
         )
   使用方法二：作为一个有序字典，将以特定神经网络模块为元素的有序字典（OrderedDict）为参数传入。
   model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
         ]))
   net = nn.Sequential()
   net[0]指Sequential里面的第一个模块，以此类推。
   net.add_module('linear1', nn.Linear(num_inputs, num_hiddens))
   net.add_module('linear2', nn.Linear(num_hiddens, num_ouputs))
44.torch.nn.Flatten(start_dim=1, end_dim=- 1),start_dim和end_dim，分别表示开始的维度和终止的维度，默认值分别是1和-1，
   其中1表示第一维度，-1表示最后的维度。结合起来看意思就是从第一维度到最后一个维度全部给展平为张量。
   注意：数据的维度是从0开始的，也就是存在第0维度，第一维度并不是真正意义上的第一个。看源代码。
45.net.apply(),pytorch的任何网络net，都是torch.nn.Module的子类,都算是module。pytorch中的model.apply(fn)会递归地将 函数fn 应用到
   父模块的每个子模块submodule，也包括model这个父模块自身。如果要对某些特定的子模块submodule做一些针对性的处理，
   可以加入type(m) == nn.Linear:这类判断语句，从而对子模块m进行处理，看源码。模型的参数的权重w其实也是按行存储的。
   
   import torch.nn as nn
   import torch
   @torch.no_grad()
   def init_weights(m):
       print(m)
       if type(m) == nn.Linear:
           m.weight.fill_(1.0)
           print(m.weight)
   net = nn.Sequential(nn.Linear(4,2,bias=False), nn.Linear(2, 3,bias=True))
   print(net)
   print('isinstance torch.nn.Module',isinstance(net,torch.nn.Module))
   print(' ')
   net.apply(init_weights)
   输出为：
   Sequential(
     (0): Linear(in_features=4, out_features=2, bias=False)
     (1): Linear(in_features=2, out_features=3, bias=True)
   )
   isinstance torch.nn.Module True

   Linear(in_features=4, out_features=2, bias=False)
   Parameter containing:
   tensor([[1., 1., 1., 1.],
           [1., 1., 1., 1.]], requires_grad=True)
   Linear(in_features=2, out_features=3, bias=True)
   Parameter containing:
   tensor([[1., 1.],
           [1., 1.],
           [1., 1.]], requires_grad=True)
   Sequential(
     (0): Linear(in_features=4, out_features=2, bias=False)
     (1): Linear(in_features=2, out_features=3, bias=True)
   )

46.torch.nn.CrossEntropyLoss(weight=None, reduction='mean', label_smoothing=0.0)
   weight(Tensor, optional)：如果输入这个参数的话必须是一个1维的tensor，长度为类别数C，每个值对应每一类的权重.
   reduction (string, optional) ：指定最终的输出类型，有'none','mean'输出结果平均值,'sum'输出结果求和,默认为'mean'。
   输入的input必须是原始数据，不能是经过softmax或者normalized或者one—hot编码,原因是这个函数会首先对输入的原始得分进行softmax.
   输入的target也就是label也不能是one-hot编码，必须直接是原本的类比如[5,3,1,1,0]，这五个样本分别对应其类。
   注意写的过程中不能直接写loss = nn.CrossEntropyLoss(x_input, x_target)，必须要像下面那样先把求交叉熵的库函数定义到一个自己命名的函数再调用，
   否则会报错。RuntimeError: bool value of Tensor with more than one value is ambiguous。
   loss_fn = nn.CrossEntropyLoss()
   x_input = torch.randn(2, 3)
   x_target = torch.tensor([0, 2])
   loss = loss_fn(x_input, x_target)

   torch.nn.MSELoss(reduction=None)计算输入与输出的均方,没有开根。它本身的参数没定义输入和目标，而是在类里面定义的。
   reduction，默认None，还有均值'mean',求和'sum'。
   loss = torch.nn.MSELoss()
   input = torch.randn(3, 5, requires_grad=True)
   target = torch.randn(3, 5)
   output = loss(input,target)
   output.backward()
47.torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)
   params（必须参数）: 这是一个包含了需要优化的参数（张量）的迭代器，例如模型的参数 model.parameters()。
   momentum（默认值为 0）: 动量（momentum）是一个用于加速 SGD 收敛的参数。它引入了上一步梯度的指数加权平均。通常设置在 0 到 1 之间。
                         当 momentum 大于 0 时，算法在更新时会考虑之前的梯度，有助于加速收敛。
   dampening（默认值为 0）: 阻尼项，用于减缓动量的速度。在某些情况下，为了防止动量项引起的震荡，可以设置一个小的 dampening 值。
   weight_decay（默认值为0:权重衰减也称为L2正则化项它用于控制参数的幅度以防止过拟合。通常设置为一个小的正数(范围为[0.0001,0.001]，通常为`1e-4`)。
   nesterov（默认值为 False）: Nesterov 动量。当设置为 True 时，采用 Nesterov 动量更新规则。Nesterov 动量在梯度更新之前先进行一次预测，
                             然后在计算梯度更新时使用这个预测。

   微调(单独对某一层或某些层进行设置)官方文档有写：
   传递一个可迭代的字典，它们中的每一个字典都将定义一个单独的参数组，并且应该包含一个params键，其中包含属于该参数组的参数列表。
   其他键(如'lr')应该与优化器接受的关键字参数匹配，并将用作该组的优化选项
   optim.SGD([
                {'params': model.base.parameters(), 'lr': 1e-2},
                {'params': model.classifier.parameters()}
            ], lr=1e-3, momentum=0.9)
   这意味着model.base的参数将使用0.01的学习率，model.classifier的参数将使用默认学习率0.001，最后动量0.9将应用于整个模型。
48.model.parameters()返回每层网络的权重和偏置，但是直接print是不会显示内容的，而是一段地址，需要将迭代器转化为列表显示。
   print(list(net.parameters()))
49.numpy.power(x,y)其中的x和y都可以包含多个数据，比如列表，但是前后个数得对应，它会按相同索引位置进行x的y次方运算
   numpy.power([1,2,3],[2,2,2])
   输出[1,4,9]
   当对矩阵进行运算时，会拿x矩阵的第一行依次进行y中数的次方，然后再拿x矩阵的第二行依次进行y中数的次方
   x=[[1],[2],[3],[4]]
   print(np.power(x,np.arange(3)))
   得到
   [[ 1  1  1]  #1的0，1，2次方
    [ 1  2  4]  #2的0，1，2次方
    [ 1  3  9]  #3的0，1，2次方
    [ 1  4 16]] #4的0，1，2次方
50.math.gamma(n)=(n-1)!注意是计算传入值n的n-1的阶乘
51.均匀分布：返回一个张量，包含了从区间[0，1)的均匀分布中抽取的一组随机数，size定义了输出张量的形状
      torch.rand(*size,out=None)
   标准正态分布：返回一个张量，包含了从均值为0，方差为1的高斯函数中抽取的一组随机数
      torch.randn(*size,out=None,dtype=None)
      torch.randn_like(input,dtype=None)生成一个和input同型的随机抽取标准正态分布
   离散正太分布：返回一个张量，包含从给定参数means(Tensor),std(Tensor)的离散正态分布中抽取随机数。
      torch.normal(means, std, out=None)
      和randn的区别：normal可以自己设定均值和标准差。

   torch.randperm(n, out=None) → 给定参数n，返回一个从[0, n -1) 的随机整数排列,建议使用torch.arange()
52.super(Net, self).__init__()是指首先找到Net的父类（比如是类NNet），然后把类Net的对象self转换为类NNet的对象，
   然后“被转换”的类NNet对象调用自己的init函数，其实简单理解就是子类把父类的__init__()放到自己的__init__()当中，
   这样子类就有了父类的__init__()的那些东西。子类继承了父类的所有属性和方法，父类属性自然会用父类方法来进行初始化。
53.神经网络的搭建：设计神经网络的核心：构造函数__init__和forward方法
   注意事项：
   我们在定义自已的网络的时候，需要继承nn.Module类，并重新实现构造函数__init__和forward这两个方法。
   1.一般把网络中具有可学习参数的层（如全连接层、卷积层等）放在构造函数__init__()中，所有放在__init__里面的层的都是这个模型的“固有属性。
   2.一般把不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)放在构造函数中
   3.forward方法是必须要重写的，它是实现模型的功能，实现各个层之间的连接关系的核心
   import torch
   class MyNet(torch.nn.Module):
       # 重写构造函数，构造函数也被称为构造器，当创建对象的时候第一个被自动调用的函数
       # 一旦创建对象，则会自动调用该构造函数，完成对象的属性的初始化设置
       def __init__(self):

           #第一步：调用父类的构造函数，以继承父类的一些属性
           super(MyNet, self).__init__()  
           
           #第二步，增加一些属性，比如Conv2d卷积层、ReLU激活函数层、MaxPool2d池化层
           # 通过self.属性名的方式，给创建的初始化实例对象增加一些属性
           self.conv = torch.nn.Conv2d(3, 32, 3, 1, 1)
           self.relu=torch.nn.ReLU()
           self.max_pooling=torch.nn.MaxPool2d(2,1)
    
           self.dense = torch.nn.Linear(32 * 3 * 3, 10)
    
   
           #第三步：覆写forward方法，实现前面定义的各个层的顺序连接，也就是完成来了前向传播。
       def forward(self, x):
           x = self.conv1(x)
           x = self.relu1(x)
           x = self.max_pooling(x)
           x = self.dense(x)
           # 前向传播结束后，返回最终得到的计算值
           return x
   
           #第四步：创建一个对象，该对象会自动调用构造函数__init__()，就是说该对象已经被添加了一些层的属性 
   model = MyNet()
   print(model)

   搭建神经网络的不同方式：
   1.直接使用nn.Sequential打包不同的层
   import torch.nn as nn
   from collections import OrderedDict
   class MyNet(nn.Module):
       def __init__(self):
           super(MyNet, self).__init__()
           # 将卷积层配套打包
           self.conv_block = nn.Sequential(
               nn.Conv2d(3, 32, 3, 1, 1),
               nn.ReLU(),
               nn.MaxPool2d(2))
           
           # 将全连接层页配套打包
           self.dense_block = nn.Sequential(
               nn.Linear(32 * 3 * 3, 128),
               nn.ReLU(),
               nn.Linear(128, 10)
           )
       # 在这里实现了打包之后的块（层）之间的连接关系，就是前向传播
       def forward(self, x):
           conv_out = self.conv_block(x)
           res = conv_out.view(conv_out.size(0), -1)
           out = self.dense_block(res)
           return out
 
   model = MyNet()
   print(model)
   当要使用MyNet()，也即往里面传数据的时候，实际上完整写法是MyNet.forward(X)，但是一般是MyNet(X)，因为python使用魔法__call__

   2.通过OrderedDict中的元组方式打包层，同时可以给各层命名
   import torch.nn as nn
   from collections import OrderedDict
   class MyNet(nn.Module):
      def __init__(self):
         super(MyNet, self).__init__()
         
         # 注意这里可以算是一个完整的卷积块，这样看起来是不是更方便一些
         self.conv_block = nn.Sequential(
               OrderedDict(
                  [
                     ("conv1", nn.Conv2d(3, 32, 3, 1, 1)),
                     ("relu1", nn.ReLU()),
                     ("pool", nn.MaxPool2d(2))
                  ]
               ))
            
               # 这里可以算是一个完整的全连接块
         self.dense_block = nn.Sequential(
               OrderedDict([
                  ("dense1", nn.Linear(32 * 3 * 3, 128)),
                  ("relu2", nn.ReLU()),
                  ("dense2", nn.Linear(128, 10))
               ])
         )
   
      def forward(self, x):
         conv_out = self.conv_block(x)
         res = conv_out.view(conv_out.size(0), -1)
         out = self.dense_block(res)
         return out
   
   model = MyNet()
   print(model)

54.梯度爆炸的问题：超出值域，对16位浮点数尤为严重；对学习率敏感：学习率太大 → 大参数值 → 更大的梯度；学习率太小 → 训练无进展，需要在训练过程中不断调整学习率。
   梯度消失的问题：梯度值为零；训练没有进展；对底部层尤为严重，仅仅顶部层训练的好，无法让神经网络更深。
55.协变量偏移(输入x的变化导致标签y的变化)：输入分布P(x)随时间而改变，而条件分布P(y|x)不变则称为协变量偏移，根据公式P(y)肯定也变化。
   标签偏移(标签y的变化导致输入x的变化)：与协变量偏移相反，边缘分布P(y)改变，而条件分布P(y|x)在不同领域不变。预测患者的疾病，我们可能根据症状来判断，
   即使疾病的相对流行率随着时间的推移而变化。
   概念偏移：当标签的定义发生变化时就会出现这种问题。
   非平稳分布：当分布变化缓慢并且模型没有得到充分更新时就会出现
56.批量学习：对一批数据进行训练，训练完成后，就直接应用基本不会更新。
   在线学习：首先观测到Xi然后我们得出一个估计值发f(Xi),只有当我们做到这一点后，我们才观测到yi然后根据我们的决定，我们会得到奖励或损失。
   控制：将PID应用于自动调整超参数。
   强化学习：强调如何基于环境而行动，以取得最大化的预期利益。
57.type()是python内置函数，适用于任何python对象，包括但不限于pandas的dataframe和series。返回数据结构类型(list,dict,numpy)
   dtype()是pandas DataFrame和Series对象的属性。返回数据元素的数据类型(int,float...)。
      对于DataFrame，dtypes返回一个series，其中包含dataframe中每列的数据类型。
      对于Series dtypes返回该Series数据的单一类型。没什么意外的话每个Serise里得数据类型应该是一样的。
      注意：由于列表和字典可以包含不同类型的对象，所以无法调用dtype，而是调用type
   astype()改变numpy.array中所有数据元素的数据类型
58.pandas.Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)，类似一维数组
   Series是一维的；都有一个索引，索引默认从0开始的整数；在创建后大小不变，但可通过append等操作改变；支持数学运算、统计分析、字符串处理等操作。
   Series.index
   Series.values可以访问它里面的值
   Series可以包含缺失值，并用NAN来表示缺失或无值
   s=pd.Series({'a':1, 'b':2, 'c':3})
   输出:
   a  1
   b  2
   c  3
59.pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)，类似二维数组
   Dataframe是pandas中的数据结构，类似于excel，可以设置列名和行名
   dataframe.index()获得行索引
   dataframe.columns()获得列索引
   dataframe.values获得里面的值
   1.使用字典创建，默认生成整数索引, 字典的键作列,值作行，如果只需要部分字典数据，那么只需在index中指定想要的即可。
   data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
           'year': [2000, 2001, 2002, 2001, np.nan],  # np.nan表示NA
           'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}
   DataFrame(data)
   输出：
       state	year	pop
   0	Ohio	  2000.0	1.5
   1	Ohio	  2001.0	1.7
   2	Ohio	  2002.0	3.6
   3	Nevada  2001.0	2.4
   4	Nevada  NaN	2.9   
   2.也可使用嵌套字典创建，最外层的键作为行名，内层的键作为列名
   3.传递一个numpy array，时间索引以及列标签来创建一个DataFrame
   data = DataFrame(np.arange(10,26).reshape((4, 4)),
                 index=['Ohio', 'Colorado', 'Utah', 'New York'], 
                 columns=['one', 'two', 'three', 'four'])
   输出：
   	      one	two  three	four
   Ohio	   10	   11	  12	  13
   Colorado	14	   15	  16	  17
   Utah	   18	   19	  20	  21
   New York	22	   23	  24	  25   
60.pandas.concat( objs, axis=0)
   objs是要合并的对象，比如df1,df2,传递的时候把二者放进小括号或者中括号里都行[df1.df2]或(df1,df2)
   axis是要合并的轴，默认是axis=0,即按行合并也就是上下堆叠

   torch.cat(tensors, dim=0, out=None)
   和python中的内置函数cat()， 在使用和目的上，是没有区别的，区别在于前者操作对象是tensor
   注意：原tensor不能是空的，要连接的tensor维度要和原tensor一致
   x_train, y_train = None, None #必须是None而不能是'[]'用来初始化
   if x_train is None:
      x_train = x_part
   else:
      x_train = torch.cat([x_train,x_part],0)

   torch.stack(input,dim)沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状
   列如，把多个二维矩阵连接成一个三维矩阵
   T1 = torch.tensor([[1, 2, 3],
        		[4, 5, 6]])
   T2 = torch.tensor([[10, 20, 30],
        		[40, 50, 60]])
   R0 = torch.stack((T1, T2), dim=0)
61.hashlib加密
   包括hashlib.md5()将任意长度的信息转换为一个 128 位的哈希值
   hashlib.sha1()比md5安全性更高，其他一样
   hashlib.256()将任意长度的信息转换为一个 256 位的哈希值
   hashlib.512()将任意长度的信息转换为一个 512 位的哈希值
   import hashlib
   hash_object = hashlib.md5()
   hash_object.update(b'Hello World') # b'Hello World' 表示将字符串 'Hello World' 转化为二进制数据
   print(hash_object.hexdigest())
62.pandas.get_dummies(data, dummy_na=False，dtype=None)将分类变量转化为虚拟变量，如独热编码。
   进行独热编码时出现TypeError: can't convert np.ndarray of type numpy.object_.就要指定dtype值
   某些ide独热编码时会将数据处理成boolean类型，需要指定处理类型为数值型，如int.
   并且有的ide会把NAN和空值当成一样的来进行处理，如果要想让ide知道NAN是有意义的，就需要先对NAN进行转换，转换成任何一个其他标记
63.DataFrame.fillna(value=None, axis=None, inplace=False)
   value:用于填充缺失值的值，可以是标量、字典、Series 或 DataFrame
   axis：指定在哪个轴上执行填充操作。
   inplace：是否在原 DataFrame 上直接进行修改。
64.torch.clamp(input, min, max, out=None)
   将input的值限制在[min, max]之间，并返回结果。常用的有1，float('inf')等。
65.slice(start, stop, step)是Python内置的一个函数，用于生成一个切片对象。
   s = "Hello, World!"
   print(s[slice(0, 5)])  # 输出 "Hello"
   print(s[slice(0, 12, 2)])  # 输出 "HloWrd"
66.nn.init.里包含很多用于初始化的函数
   nn.inin.normal_()正态初始化
   nn.init.zeros_()全零初始化
   nn.init.constant_()常数初始化
   nn.init.xavier_uniform_(m.weight,mean,std)通过网络层时，使输入和输出的方差相同，避免梯度消失和梯度爆炸，包括前向传播和后向传播。
   nn.init.uniform_(m.weight,mean,std)
67.保存张量和加载张量
   torch.save(obj,str),obj就是要存储的参数，str是想保存的文件名
   torch.load(str)加载名为str的文件
   保存模型和加载模型
   torch.save(net.state_dict(), 'mlp.params')
   当要加载参数的时候必须把网络给复现了(也就是复制过来)
   clone = MLP()
   clone.load_state_dict(torch.load('mlp.params'))
68.nn.MaxPool2d(3) 3x3的最大池化层