@1.conda的操作
  1. conda --version #查看conda版本，验证是否安装
  2. conda update conda #更新至最新版本，也会更新其它相关包
  3. conda update --all #更新所有包
  4. conda update package_name #更新指定的包
  5. conda create -n env_name package_name #创建名为env_name的新环境，并在该环境下安装名为package_name 的包，
   可以指定新环境的版本号。例如：conda create -n python2 python=python2.7 numpy pandas，
   创建了python2环境，python版本为2.7，同时还安装了numpy pandas包
   也可conda create --name d2l python=3.9 -y 就单纯创建一个名叫d2l的环境，并装有3.9的python
  6. conda activate env_name #切换至env_name环境
  7. conda deactivate #退出环境
  8. conda info -e #显示所有已经创建的环境
  9. conda create --name new_env_name --clone old_env_name #复制old_env_name为new_env_name
  10. conda remove --name env_name –all #删除环境
  11. conda list #查看所有已经安装的包
  12. conda install package_name #在当前环境中安装包
  13. conda install --name env_name package_name #在指定环境中安装包
  14. conda remove -- name env_name package #删除指定环境中的包
  15. conda remove package #删除当前环境中的包
  16. conda env remove -n env_name #采用第10条的方法删除环境失败时，可采用这种方法
@2.将远程端口映射到本地，这样本地就可以打开远端的地址了
   ssh -L8888:localhost:8888 ubuntu@用户名
@3.norm默认求L2范数
@4.神经网络层数不计入输入层。每个输入都与每个输出相连，我们将这种变换称为全连接层（fully-connected layer）
   或称为稠密层（dense layer）
@5.要看某个向量或者矩阵a的形状时，a.shape,没有括号，因为shape是属性，可用中括号读取指定维度的大小,shape[1]就是读取轴1的形状。
   也可项shape传递具体矩阵这时候要加括号shape([1,2]).
   a.size()也可查看形状，size是函数所以要加括号。

   维度(dim)和轴(axis)，只有指定的维度是可变的，其他都是固定的，numpy用axis，pytorch用dim，二者其实是一样的
     axis：0轴从第一层向最深层方向
         1轴沿着0行到1行的方向垂直向下(纵向),按行计算(列不变)，得到列的性质；
         2轴沿着0列到1列的方向水平延伸(横向),按列计算，得到行的性质
     dim：正数就从外面往里数，第一层是0，负数就从里往外数，第一层也是
@6.如果a是3x2的矩阵，len(a)返回的是矩阵的行数。矩阵[p,m,n]其中p代表p个切片，m代表m行，n代表n列。
@7.batch_size其实越小对损失函数的收敛越有益，因为对于深度神经网络是很复杂的，当batch_size比较小的时候，
  在同一批次中噪音比例就会变大，而噪音对神经网络是有好处的，它可以增加算法鲁棒性
@8.全连接层在Linear类中定义；计算均方误差使用的是MSELoss类；SGD(小批量随机梯度下降算法)在optim模块中实现；
@9.损失函数和优化函数的联合过程：损失函数仅仅用来计算预测与真实之间的差距大小，而优化函数可以调整拟合函数从而使损失函数的值变小。
   对于一个优化函数，要优化的是拟合函数中的某些变量，比如截距b，我们可以先初始化b为0，然后计算得到b=0时损失函数的具体值，
   (横坐标的b，纵坐标是损失函数总值)通过对损失函数进行求导得到b=0时的斜率，然后对斜率乘以一个学习率，用b-lr*斜率，就可以得到一个新的b，
   然后使用新的b，又可以计算得到一个具体的损失函数值，然后...   讲解https://www.bilibili.com/video/BV1XW421F7M4
@10.当我们在一个对象上调用内置的len()函数时，实际上是在调用该对象的__len__()方法。这个方法的主要作用是返回一个对象的长度。
@11.model.zero_grad()的作用是将所有模型参数的梯度置为0。
   optimizer.zero_grad()的作用是清除所有可训练的torch.Tensor的梯度。
   zero_grad()清除梯度
@12.于对抗过拟合的技术称为正则化（regularization）
@13.影响模型泛化的因素:
   1. 可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合
   2. 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
   3. 训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。
@14.自定义网络时class类里面的forward函数有什么用？
2.矩阵a的截取：a[起始行：结束行，起始列：结束列]，如果只截取某2列为a[,1]，因为第二列的索引是1，
   a[2]只截取第三行
   还有a[起始行：结束行：间隔行，起始列：结束列：间隔列],如a[::3,::2]，从第零行开始，每三行一跳，每两列一跳.
   高级索引:在numpy或pytorch等框架中对张量的操作不只有类似列表的切片索引等操作，还提供了高级索引,整数数组索引、布尔索引及花式索引。
   整数数组索引：
   x=[[0,1,2]
      [3,4,5]
      [6,7,8]]
   1.取出其(0,0)，(1,1)和(2,0)位置处的元素,给出两个一维数组分别指定第0维和第1维,x[[0,1,2],[0,1,0]]
   2.传入的位置指定数组也可为多维，取出其[[(0,0),(0,2)]，[(3,0),(3,2)]]的元素
   rows = torch.tensor([[0,0],[3,3]]) 
   cols = torch.tensor([[0,2],[0,2]])
   y = x[rows,cols]

3.矩阵切片loc和iloc。loc函数：通过行索引 "Index" 中的具体值来取行数据（如取"Index"为"A"的行）
   iloc函数：通过行号来取行数据（如取第二行的数据）。
#创建一个Dataframe
data=pd.DataFrame(np.arange(9).reshape(3,3),index=list('abc'),columns=list('ABC'))
Out: 
    A   B   C
a   0   1   2
b   4   5   6
c   8   9  10
#取索引为'a'的行
In: data.loc['a']
Out:
A    0
B    1
C    2
#取第一行数据，索引为'a'的行就是第一行，所以结果相同
In: data.iloc[0]
Out:
A    0
B    1
C    2
In:data.loc[['a','b'],['A','B']] #提取index为'a','b',列名为'A','B'中的数据
Out: 
   A  B
a  0  1
b  4  5
In:data.iloc[[0,1],[0,1]] #提取第0、1行，第0、1列中的数据
Out: 
   A  B
a  0  1
b  4  5

4.item()的作用是取出单元素张量的元素数值并返回该数值，保持原元素类型不变。
  a=torch.tensor([3.5])
  a.item()
  就可以得到数值3.5而不是向量[3.5]

5.os.path.join(path, *paths)
  path：代表文件系统路径的path-like对象。
  *paths：代表文件系统路径的path-like对象。它表示要连接的路径组件。
  从后往前看，会从第一个以'/'开头的参数开始拼接，之前的参数全部丢弃；以上一种情况为先。在上一种情况确保情况下，
  若出现'./'开头的参数，会从'./'开头的参数的前面参数全部保留。如果最后一个组件为空，则生成的路径以一个’/’分隔符结尾。

  print("1:", os.path.join('aaaa', '/bbbb', 'ccccc.txt'))
  print("2:", os.path.join('/aaaa', '/bbbb', '/ccccc.txt'))
  print("3:", os.path.join('aaaa', 'bbbb', './cccc', 'ccccc.txt'))
  print("4:", os.path.join('aaaa', 'bbbb', './cccc', '/dddd', 'ccccc.txt'))
  print("5:", os.path.join('aaaa', 'bbbb', 'cccc', 'dddd', 'ccccc.txt'))
  print("6:", os.path.join('aaaa', 'bbbb', 'cccc', 'dddd', ''))
  输出结果为：
   1: /bbbb/ccccc.txt
   2: /ccccc.txt
   3: aaaa/bbbb/./cccc/ccccc.txt
   4: /dddd/ccccc.txt
   5: aaaa/bbbb/cccc/dddd/ccccc.txt
   6: aaaa/bbbb/cccc/dddd/
6.os.mkdir(path,mode),mode就是文件权限4，2，1，代表读，写，执行。
  mkdir只能创建一个目录，如果路径中存在两个要创建的目录会报错
  os.makedirs(path,mode，exist_ok=False)递归创建目录，exist_ok：如果已经存在怎么处理，默认是 False ，
  即：已经存在程序报错。当为 True 时，创建目录的时候如果已经存在就不报错。
7.pytorch创建的张量默认是行向量，0轴--行，1轴--列，2轴--深度
8.torch.noraml(means, std, out=None)返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数。 
  均值means是一个张量，包含每个输出元素相关的正态分布的均值。 std是一个张量，
  包含每个输出元素相关的正态分布的标准差。 均值和标准差的形状不须匹配，但每个张量的元素个数须相同。
9.矩阵运算@，*，tensor.dot()，torch.mul()，torch.mm()，torch.mv()，tensor.t()，torch.matmul()
  @表示常规的数学上定义的矩阵相乘
  *表示两个矩阵对应位置处的两个元素相乘
  torc.dot(x,y): x和y对应位置相乘最后相加,x，y均为一维向量，和np.dot()不同，不能是多维矩阵，
  torch.mul()等同*:表示相同shape矩阵点乘，即对应位置相乘，得到矩阵有相同的shape，可用广播机制。
  torch.mm(a, b)等同@：正常矩阵相乘，要求a的列数与b的行数相同。
  torch.mv(X, w0):矩阵和向量相乘.第一个参数是矩阵，第二个参数只能是一维向量,等价于X乘以w0的转置
  Y.t():矩阵Y的转置。
  torch.matmul(input, other) → Tensor，计算两个张量input和other的矩阵乘积，可利用广播机制进行不同维度的相乘操作。
10.tensor.detach()返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,
   不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。
11.matplotlib允许我们将一个figure通过栅格系统划分成不同的格子，然后在格子中画图，这样就可以在一个figure中画多个图了。
  这里的每个格子有两个名称：Axes和subplot。subplot是从figure所有的格子来看的。因为figure要统一管理协调这些格子的位置、
  间隔等属性，管理协调的方法和属性设置就在subplots的层面进行。Axes是从作为画图者的我们的角度来定义的，我们要画的点、线等
  都在Axes这个层面来进行。画图用的坐标系统自然也是在Axes中来设置的。
  如果将Matplotlib绘图和我们平常画画相类比，可以把Figure想象成一张纸（一般被称之为画布），Axes代表的则是纸中的一片区域，axis是轴。
  plt绘图：第一种方式的代码来看，先生成了一个Figure画布，然后在这个画布上隐式生成一个画图区域进行画图。
  plt.figure()
  plt.plot([1,2,3],[4,5,6])
  plt.show()
  ax绘图：第二种方式同时生成了Figure和axes两个对象，然后用ax对象在其区域内进行绘图
  fig,ax = plt.subplots()
  ax.plot([1,2,3],[4,5,6])
  plt.show()
  也就是说fig, ax = plt.subplots()是fig = plt.figure()
                                  ax = fig.add_subplot(111)
   的简写

  plot()：用于绘制线图和散点图
  scatter()：用于绘制散点图
  bar()：用于绘制垂直条形图和水平条形图
  hist()：用于绘制直方图
  pie()：用于绘制饼图
  imshow()：用于绘制图像
  subplot():用于创建子图
  subplots()：用于创建子图

  plt.scatter(x, y, s=None, c=None, marker=None,alpha=None, linewidths=None,edgecolors=None)
  x, y → 散点的坐标
  s → 散点的面积,也就是画的每个点的面积
  c → 散点的颜色（默认值为蓝色，'b'，其余颜色同plt.plot( )）
  marker → 散点样式（默认值为实心圆，'o'，其余样式同plt.plot( )）
  alpha → 散点透明度（[0, 1]之间的数，0表示完全透明，1则表示完全不透明）
  linewidths →散点的边缘线宽
  edgecolors → 散点的边缘颜色
  xlim,ylim → 设置x轴和y轴的极限
  fmt = '[marker][line][color]' 
      https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html
       线型  '-'：实线  '--'：虚线  '-.'：点画线  ':'：点线
       点型  
       颜色
  详情：https://zhuanlan.zhihu.com/p/111331057

  ax = plt.gca() #获取当前坐标轴
  ax.set_title('第一张图')
  ax.set_ylabel('T(C)')
  ax.set_xlabel('Time')
  ax.legend()
  ax.axes.get_xaxis().set_visible(False)隐藏x坐标轴
  ax.axes.get_yaxis().set_visible(False)隐藏y坐标轴

  ax.flatten()是对多维数据的降维函数。
  ax.flatten(),默认缺省参数为0，也就是说flatten()和flatten(0)效果一样。python里的flatten(dim)表示，从第dim个维度开始展开，
  将后面的维度转化为一维.也就是说，只保留dim之前的维度，其他维度的数据全都挤在dim这一维。
12.
  画布参数plt.figure：
  fig=plt.figure(figsize=(4,3),facecolor='blue')  # 4*3 英寸，蓝色背景

  plt.ioff()  # 关闭画图的窗口
  plt.cla  #清除上一幅图像

  图例注释plt.legend()让标签显示出来：
  plt.legend(loc=3) #写3和'lower left'都可以

  创建子图subplot或subplots：
  区别：subplots() 既创建了一个包含子图区域的画布，又创建了一个 figure 图形对象，而 subplot() 只是创建一个包含子图区域的画布。
  subplot(nrows, ncols, index)
  subplots(nrows, ncols,sharex=False, sharey=False，figsize)
  示例：
  plt.subplot(221) #两行两列的第1个子图
  plt.plot(x,y)对第一个子图进行绘画
  plt.subplot(222) #两行两列的第2个子图
  plt.plot(x,y)对第二个子图进行绘画

  fig, axs = plt.subplots(2, 2)
  axs[0]代表的就是第一行两个子区域
  axs[0][0].plot(x, y) 或 axs[0,0]  对第一个子图进行绘画
  axs[0,1].plot(x,y) 对第二个子图进行绘画
  axs[1][1].scatter(x, y) 或 axs[1,1].scatter(x,y) 对第四个子图进行绘画
  不想用axs[,]可以
  f, ([ax1, ax2],[ax3,ax4]) = plt.subplots(2, 2, sharex=True)
  直接指定每一块区域的名字，然后直接用ax1，ax2，ax3，ax4来指定操作区域即可
13.向tensor矩阵传递向量索引，可以得到向量索引中的每个元素对应矩阵的行，A=tensor([[1,2,3],[4,5,6],[7,8,9]]),
   b=tensor([0,1]),c=tensor([1,2])，d=tensor([0,2])
   A[b]:tensor([[1,2,3],[4,5,6]])，得到A矩阵的第零行和第一行
   A[c]:tensor([[4,5,6],[7,8,9]])
   A[d]:tensor([[1,2,3],[7,8,9]])，得到矩阵A的第零行和第二行
14.with torch.no_grad():  表示所有计算得出的tensor的requires_grad都自动设置为False，反向传播时就不会自动求导了，从而节省计算开支。
15.zip()函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。
   如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表
   a = [1,2,3]
   b = [4,5,6,7,8]
   zipped = zip(a,b)     # 打包为元组的列表
输出为：[(1, 4), (2, 5), (3, 6)]
   zip(*zipped)    # 与 zip 相反，*zipped 可理解为解压，返回二维矩阵式
输出为：[(1, 2, 3), (4, 5, 6)]
16.data.TensorDataset() 以用来对tensor进行打包，就好像python中的zip功能。该类通过每一个tensor的第一个维度进行索引。
   因此，该类中的 tensor 第一维度必须相等. 另外：TensorDataset 中的参数必须是 tensor.
17.TensorDataset(tensor1,tensor2)-->[[tensor1，label1]，[tensor2,label2],...]
   from torch.utils import data
   a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9]])
   b = torch.tensor([44, 55, 66, 44, 55, 66, 44, 55, 66])
   # TensorDataset对tensor进行打包
   train_ids = data.TensorDataset(a, b)
   #打包后的列表里的每一个对象的形式为(tensor([4, 5, 6]), tensor(55))
   for x_train, y_label in train_ids:
      print(x_train, y_label)
输出结果为：
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)

18.data.DataLoader()就是用来包装所使用的数据，把训练数据分成多个小组，每次抛出一批数据.
dataloader进行数据封装,就是拿出批次大小的数据，里面的内容类似于dataset()后的形式，batch_size=1时是[(数据1，标签1)，(数据2，标签2),]，
batch_size=2时是[[tensor[数据1，数据2]，tensor[标签1，标签2]]，tensor[[数据3，数据4]，tensor[标签3，标签4]]]
即一个对象(是个列表)里面包含batch_size个样本，数据又分为数据向量矩阵和标签向量矩阵
[tensor([[4, 5, 6],
        [4, 5, 6],
        [1, 2, 3]]), tensor([55, 55, 44])]
train_loader = DataLoader(dataset=train_ids, batch_size=3, shuffle=True)
for i, data in enumerate(train_loader, 1):  
#enumerate(sequence,[start=0])，默认序号从0开始数，就比如这个指定了1，就从batch：1即从序号1开始数了，实际并不影响数据，仅仅只是计数而已。
# 注意enumerate返回值有两个,一个是序号，一个是数据（包含训练数据和标签）。
    x_data, label = data
    print(' batch:{0} x_data:{1}  label: {2}'.format(i, x_data, label))
输出结果为：
 batch:1 x_data:tensor([[7, 8, 9],
        [1, 2, 3],
        [4, 5, 6]])  label: tensor([66, 44, 55])
 batch:2 x_data:tensor([[4, 5, 6],
        [4, 5, 6],
        [1, 2, 3]])  label: tensor([55, 55, 44])
 batch:3 x_data:tensor([[1, 2, 3],
        [7, 8, 9],
        [7, 8, 9]])  label: tensor([44, 66, 66])
19.独热编码（one-hot encoding）,独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。
20.交叉熵损失（cross-entropy loss），它是分类问题最常用的损失之一。交叉熵从P到Q，记为H(P,Q)。可以把交叉熵想象为“主观概率
   为Q的观察者在看到根据概率P生成的数据时的预期惊异”。当P=Q时，交叉熵达到最低。交叉熵是一个衡量两个概率分布之间差异的很好的度量，
   它测量给定模型编码数据所需的比特数。

   公式：
   先对每一个样本数据中的每一类别求 -(1类真实概率*log(1类预测概率)+2类真实概率*log(2类预测概率)+...)
   然后对每一个样本计算出的结果求和再平均

   sigmoid(softmax)+cross-entropy loss 擅长于学习类间的信息，因为它采用了类间竞争机制，它只关心对于正确标签预测概率的准确性，
   忽略了其他非正确标签的差异，导致学习到的特征比较散。基于这个问题的优化有很多，比如对softmax进行改进，
   如L-Softmax、SM-Softmax、AM-Softmax等
   详情：https://www.zhihu.com/tardis/zm/art/35709485?source_id=1005
21.iter(object)用来生成迭代器，迭代器是一个可以记住遍历的位置的对象。object -- 支持迭代的集合对象。
   一类是：list、tuple、dict、set、str。二类是：generator（都是Iterator对象），包含生成器和带yield的generator function
   生成器不但可以作用于for，还可以被next函数不断调用并且返回下一个值，可以被next函数不断调用返回下一个值的对象称为迭代器（Iterator）。
22.next(iterator),iterator--可迭代的对象,iter()把目标转为可迭代的对象，然后通过next(iterator)访问每一个可迭代的对象。
23.view()相当于reshape、resize，重新调整Tensor的形状。
   v1 = torch.range(1, 16) 
   v2 = v1.view(4, 4)
24.transforms.ToTensor()这是一个图像转换的操作，将图像从PIL(Python Imaging Library)格式转换为PyTorch的Tensor类型。
   它还会将像素值的范围从[0, 255]缩放到[0, 1]。
25.transforms.Compose()就是一个把几个对图像的操作连接成一个操作的函数。具体是对图像进行各种转换操作，并用函数compose将这些转换操作组合起来。
   它其实是一个列表，按顺序记录着各种转换操作，也称为转换列表。
26.torch.argmax(input，[dim]) → LongTensor。返回输入张量中指定维度所有元素的最大值的索引，并组成一维向量tensor([1,2,0,1,])，可以看成行向量。
   不指定维度就是把矩阵展成一维向量，再返回索引。
   实例https://zhuanlan.zhihu.com/p/409912530
27.numel()函数：返回数组中元素的个数。y.numel()
28.plt.draw()函数的作用是重新绘制当前图形。如果需要清除现有图形可以使用 matplotlib.pyplot.clf() 和 matplotlib.axes.Axes.clear()。
   如果在修改图形的过程中使用了plt.show()函数，则plt.draw()函数将失效。因此，一般情况下应该先调用plt.draw()函数，然后再使用plt.show()函数。
   plt.show()将显示您正在处理的当前图形。
29.ipython中display模块中的display函数它可以显示文本、图像、音频和视频等。
   from IPython.display import display,Image，Audio, Video
   显示图像：display(Image(filename='image.png'))
   显示视频：display(Video(filename='video.mp4'))
   显示音频：display(Audio(filename='audio.mp3'))
30.np.linspace(start,stop,num,[endpoint],[dtype]),起始值，终值，生成个数。生成结果第一个就是起始值，最后一个就是终值，其余数在它们之间等间隔取。
   可选参数endpoint决定输出结果包不包含终值，dtype决定输出数的类型。
   从0 到 100，间隔为10的数值序列：np.linspace(start = 0, stop = 100, num = 11)
   输出array([  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.])
31.lambda函数的语法只包含一个语句lambda[arg1 [,arg2,.....argn]]:expression。[arg…] 和 expression 由用户自定义。
   expression是一个参数表达式，表达式中出现的参数需要在[arg......]中有定义，并且表达式只能是单行的，只能有一个表达式。
   特性：
   lambda 函数是匿名的，通俗地说就是没有名字的函数。lambda函数没有名字。
   lambda 函数有输入和输出：输入是传入到参数列表argument_list的值，输出是根据表达式expression计算得到的值。
   lambda 函数拥有自己的命名空间：不能访问自己参数列表之外或全局命名空间里的参数，只能完成非常简单的功能。
   用法1：将lambda函数赋值给一个变量，通过这个变量间接调用该lambda函数。
   add = lambda x, y: x+y  相当于定义了加法函数lambda x, y: x+y，并将其赋值给变量add，这样变量add就指向了具有加法功能的函数。
   这时我们如果执行add(1, 2)，其输出结果就为 3。
   用法2：将lambda函数赋值给其他函数，从而将其他函数用该lambda函数替换。
   time.sleep=lambda x: None   #把标准库time中的函数sleep的功能屏蔽
   time.sleep(3)	# 程序不会休眠 3 秒钟，而是因为lambda输出为None，所以这里结果是什么都不做
32.map(function, iterable, ...)函数：将可迭代对象中的每一个对象调用function函数，然后返回所有function函数输出结果的迭代器(新列表)。
   def square(x):
	   return x ** 2
   map(square, [1,2,3,4,5])
   输出[1, 4, 9, 16, 25]
   也可改为lambda匿名函数写法：
   map(lambda x: x ** 2, [1, 2, 3, 4, 5])
   提供两个列表，将其相同索引位置的列表元素进行相加：map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])
33.reduce(function, iterable[, initializer])函数会对参数序列(可迭代对象)中的元素进行累积。
   函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）
   先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个元素用 function 函数运算，最后得到一个结果。
   from functools import reduce
   def add(x, y):            
	   return x + y
   reduce(add, [1, 3, 5, 7, 9])    # 计算列表元素和：1+3+5+7+9
   输出25
   也可改成lambda写法：
   reduce(lambda x, y: x + y, [1, 2, 3, 4, 5])
   当要把序列 [1, 3, 5, 7, 9] 变换成整数 13579，reduce就很好用：
   from functools import reduce
   def fn(x, y):
	   return x * 10 + y
   reduce(fn, [1, 3, 5, 7, 9])
   输出13579
34.sorted(iterable, key=None, reverse=False)
   sort和sorted的区别：sort是list的一个方法，而sorted可以对所有可迭代的对象进行排序操作。
   list 的 sort 方法返回的是对已经存在的列表进行操作，无返回值，而内建函数sorted方法返回的是一个新的list，而不是在原来的基础上进行的操作。
   key：用来进行比较的元素，只有一个参数，参数取自可迭代对象
   reverse：是排序规则，reverse = True 降序 ， reverse = False 升序（默认）
   L=[('b',2),('a',1),('c',3),('d',4)]
   sorted(L, key=lambda x:x[1])
   输出[('a', 1), ('b', 2), ('c', 3), ('d', 4)]
35.filter(function, iterable)用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。该接收两个参数，
   第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。
   newlist = filter(lambda x: x % 3 == 0, [1, 2, 3])
   print(list(newlist)) 输出[3]
36.hasattr(object, name)object --> 对象；name --> 字符串，属性名。
   用于判断对象是否包含对应的属性
37.assert expression,'自定义报错语句'，在表达式条件为false时触发异常。在条件不满足程序运行条件时直接返回错误，而不必等待程序运行后出现崩溃的情况.
   它主要是用来检测调试你的代码问题，当你使用 assert 来检测你的代码的时候，如果是 True ，它就会直接通过，
   当它是 False 的时候，就会抛出错误，然后你就可以根据错误进行定位，从而在具体的位置修改代码。
38.torch.zeros_like(input)和torch.ones_like(input)是生成一个和input同型的全0或全1矩阵
39.plt.rcParams[key]它是一个字典。使用rc配置文件可以自定义图形的各种默认属性，包括窗体大小、每英寸的点数、线条宽度、颜色、样式、坐标轴、
   坐标和网络属性、文本、字体等。rc参数存储在字典变量中，通过字典的方式进行访问。
40.x.grad.data.zero_(),optimizer.zero_grad()
41.optimizer.zero_grad和model.zero_grad的区别(model为自定义模型)：
   当仅有一个model，同时optimizer只包含这一个model的参数，那么model.zero_grad和optimizer.zero_grad没有区别，可以任意使用
   当有多个model，同时optimizer包含多个model的参数时，如果这多个model都需要训练，那么使用optimizer.zero_grad是比较好的方式，
   耗时和防止出错上比对每个model都进行zero_grad要更好。
   当有多个model，对于每个model或者部分model有对应的optimizer，同时还有一个total_optimizer包含多个model的参数时。
   如果是是只想训练某一个model或者一部分model，可以选择对需要训练的那个model进行model.zero_grad，然后使用他对应的optimizer进行优化。
   如果是想对所有的model进行训练，那么使用total_optimizer.zero_grad是更优的方式。
42.当向量没有进行过求导运算时进行梯度清零会报错AttributeError: 'NoneType' object has no attribute 'data'
43.nn.Sequential()一个有序的容器,里面的神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行，
   所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。
   使用方法一：作为一个有顺序的容器
   model = nn.Sequential(
      nn.Conv2d(1,20,5),
      nn.ReLU(),
      nn.Conv2d(20,64,5),
      nn.ReLU()
      )
   net = nn.Sequential(
         nn.Linear(num_inputs, num_hidden)
    # 传入其他层
         )
   使用方法二：作为一个有序字典，将以特定神经网络模块为元素的有序字典（OrderedDict）为参数传入。
   model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
         ]))
   net = nn.Sequential()
   net.add_module('linear1', nn.Linear(num_inputs, num_hiddens))
   net.add_module('linear2', nn.Linear(num_hiddens, num_ouputs))
44.torch.nn.Flatten(start_dim=1, end_dim=- 1),start_dim和end_dim，分别表示开始的维度和终止的维度，默认值分别是1和-1，
   其中1表示第一维度，-1表示最后的维度。结合起来看意思就是从第一维度到最后一个维度全部给展平为张量。
   注意：数据的维度是从0开始的，也就是存在第0维度，第一维度并不是真正意义上的第一个。看源代码。
45.net.apply(),pytorch的任何网络net，都是torch.nn.Module的子类,都算是module。pytorch中的model.apply(fn)会递归地将函数fn应用到
   父模块的每个子模块submodule，也包括model这个父模块自身。如果要对某些特定的子模块submodule做一些针对性的处理，
   可以加入type(m) == nn.Linear:这类判断语句，从而对子模块m进行处理。看源代码。

46.torch.nn.CrossEntropyLoss(weight=None, reduction='mean', label_smoothing=0.0)
   weight(Tensor, optional)：如果输入这个参数的话必须是一个1维的tensor，长度为类别数C，每个值对应每一类的权重.
   reduction (string, optional) ：指定最终的输出类型，有'none','mean'输出结果平均值,'sum'输出结果求和,默认为'mean'。
   输入的input必须是原始数据，不能是经过softmax或者normalized或者one—hot编码,原因是这个函数会首先对输入的原始得分进行softmax.
   输入的target也就是label也不能是one-hot编码，必须直接是原本的类比如[5,3,1,1,0]，这五个样本分别对应其类。
   注意写的过程中不能直接写loss = nn.CrossEntropyLoss(x_input, x_target)，必须要像下面那样先把求交叉熵的库函数定义到一个自己命名的函数再调用，
   否则会报错。RuntimeError: bool value of Tensor with more than one value is ambiguous。
   loss_fn = nn.CrossEntropyLoss()
   x_input = torch.randn(2, 3)
   x_target = torch.tensor([0, 2])
   loss = loss_fn(x_input, x_target)
47.torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)
   params（必须参数）: 这是一个包含了需要优化的参数（张量）的迭代器，例如模型的参数 model.parameters()。
   momentum（默认值为 0）: 动量（momentum）是一个用于加速 SGD 收敛的参数。它引入了上一步梯度的指数加权平均。通常设置在 0 到 1 之间。
                         当 momentum 大于 0 时，算法在更新时会考虑之前的梯度，有助于加速收敛。
   dampening（默认值为 0）: 阻尼项，用于减缓动量的速度。在某些情况下，为了防止动量项引起的震荡，可以设置一个小的 dampening 值。
   weight_decay（默认值为 0）: 权重衰减，也称为 L2 正则化项。它用于控制参数的幅度，以防止过拟合。通常设置为一个小的正数。
   nesterov（默认值为 False）: Nesterov 动量。当设置为 True 时，采用 Nesterov 动量更新规则。Nesterov 动量在梯度更新之前先进行一次预测，然后在计算梯度更新时使用这个预测。
48.model.parameters()返回每层网络的权重和偏置，但是直接print是不会显示内容的，而是一段地址，需要将迭代器转化为列表显示。
   print(list(net.parameters()))
49.numpy.power(x,y)其中的x和y都可以包含多个数据，比如列表，但是前后个数得对应，它会按相同索引位置进行x的y次方运算
   numpy.power([1,2,3],[2,2,2])
   输出[1,4,9]
   当对矩阵进行运算时，会拿x矩阵的第一行依次进行y中数的次方，然后再拿x矩阵的第二行依次进行y中数的次方
   x=[[1],[2],[3],[4]]
   print(np.power(x,np.arange(3)))
   得到
   [[ 1  1  1]  #1的0，1，2次方
    [ 1  2  4]  #2的0，1，2次方
    [ 1  3  9]  #3的0，1，2次方
    [ 1  4 16]] #4的0，1，2次方
50.math.gamma(n)=(n-1)!注意是计算传入值n的n-1的阶乘
51.